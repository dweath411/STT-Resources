---
title: 'SS23 STT481: Lab: Linear Regression and KNN'
subtitle: 02/03/2022
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


The `MASS` library contains the Boston data set, which records `medv` (median house value) for 506 neighborhoods around Boston. We will seek to predict `medv` using 13 predictors such as `rm` (average number of rooms per house), `age` (average age of houses), and `lstat` (percent of households with low socioeconomic status).

``` {r echo=TRUE}
library(MASS)
library(ISLR)
data(Boston)
names(Boston)
head(Boston)
```

# Simple Linear Regression
We will start by using the `lm()` function to fit a simple linear regression
model, with `medv` as the response and `lstat` as the predictor.
``` {r echo=TRUE}
lm.fit <- lm(medv ~ lstat, data = Boston)
```

If we type `lm.fit`, some basic information about the model is output. For more detailed information, we use `summary(lm.fit)`. This gives us p-values and standard errors for the coefficients, as well as the $R^2$ statistic and F-statistic for the model.
``` {r echo=TRUE}
lm.fit
summary(lm.fit)
names(lm.fit)
```

## Coefficients and confidence interval
We can extract the coefficients by `coef()`, and obtain a confidence interval for the coefficient estimates by `confint`.
``` {r echo=TRUE}
coef(lm.fit)
confint(lm.fit)
```

The `predict()` function can be used to produce the predictions of `medv` for a given value of `lstat`.
``` {r echo=TRUE}
predict(lm.fit,data.frame(lstat=(c(5,10,15))))
```

We will now plot `medv` and `lstat` along with the least squares regression line using the `plot()` and `abline()` functions.

There is some evidence for non-linearity in the relationship between `lstat` and `medv`. We will explore this issue later in this lab. The `abline()` function can be used to draw any line, not just the least squares regression line. To draw a line with intercept a and slope b, we type `abline(a,b)`. Below we experiment with some additional settings for plotting lines and points. The `lwd=3` command causes the width of the regression line to be increased by a factor of 3; this works for the `plot()` and `lines()` functions also. We can also use the `pch` option to create different plotting symbols.
``` {r echo=TRUE, fig.height = 3}
plot(Boston$lstat, Boston$medv)
abline(lm.fit)
abline(lm.fit,lwd=3)
abline(lm.fit,lwd=3, col="red")
plot(Boston$lstat,Boston$medv, col="red")
plot(Boston$lstat, Boston$medv, pch=20)
plot(Boston$lstat, Boston$medv, pch="+")
plot(1:20, 1:20, pch=1:20)
```

## Residual diagnostics
Next we examine some diagnostic plots. Four diagnostic plots are automatically produced by applying the `plot()` function directly to the output from `lm()`.
``` {r echo=TRUE}
par(mfrow=c(2,2))
plot(lm.fit)
```

## Identify which observation has the largest leverage statistic
On the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the `hatvalues()` function.
``` {r echo=TRUE}
par(mfrow=c(1,1))
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

## Add quadratic effect
The `lm()` function can also accommodate non-linear transformations of the predictors. For instance, given a predictor X, we can create a predictor X2 using `I(X^2)`. The function `I()` is needed since the `^` has a special meaning `I()` in a formula; wrapping as we do allows the standard usage in R, which is to raise X to the power 2. We now perform a regression of `medv` onto `lstat^2` and `lstat`.
``` {r echo=TRUE}
par(mfrow=c(2,2))
lm.fit <- lm(medv ~ lstat + I(lstat^2), data = Boston)
summary(lm.fit)
plot(lm.fit)
```
``` {r echo=TRUE}
par(mfrow=c(1,1))
plot(Boston$lstat, Boston$medv)
curve(coef(lm.fit)[1] + coef(lm.fit)[2]*x + coef(lm.fit)[3]*x^2, add = TRUE, col="blue")
```

# Multiple Linear Regression
In order to fit a multiple linear regression model using least squares, we again use the `lm()` function. The syntax `lm(y~x1+x2+x3)` is used to fit a model with three predictors, `x1`, `x2`, and `x3`. The `summary()` function now outputs the regression coefficients for all the predictors.
``` {r echo=TRUE}
lm.fit <- lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)
```

## Include interaction terms
It is easy to include interaction terms in a linear model using the `lm()` func- tion. The syntax `lstat:black` tells R to include an interaction term between `lstat` and `black`. The syntax `lstat*age` simultaneously includes `lstat`, `age`, and the interaction term `lstatÃ—age` as predictors; it is a shorthand for `lstat+age+lstat:age`.

``` {r echo=TRUE}
lm.fit <- lm(medv ~ lstat*age, data = Boston)
summary(lm.fit)
```

## Use all of the predictors
The `Boston` data set contains 13 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:
``` {r echo=TRUE}
lm.fit <- lm(medv ~ ., data = Boston)
summary(lm.fit)
```

## Exclude `age` and `indus`
What if we would like to perform a regression using all of the variables but one? For example, in the above regression output, `age` has a high p-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except `age`.
``` {r echo=TRUE}
lm.fit <- lm(medv ~ .- age, data = Boston)
summary(lm.fit)
par(mfrow=c(2,2))
plot(lm.fit)
```

## Include quadratic effect
``` {r echo=TRUE}
lm.fit2 <- update(lm.fit, ~ . + I(lstat^2), data = Boston)
summary(lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit2)
```

## Non-linear Transformations of the Predictors
(e.g.) $X^2$ using `I(X^2)`, $\sqrt{X}$ using `I(sqrt(X))`, and $\log(X)$ using `I(log(X))`
``` {r echo=TRUE}
lm.fit <- lm(medv ~ lstat + I(lstat^2), data = Boston)
summary(lm.fit)
lm.fit <- lm(medv ~ I(sqrt(lstat)), data = Boston)
summary(lm.fit)
lm.fit <- lm(medv ~ I(log(lstat)), data = Boston)
summary(lm.fit)
lm.fit <- lm(log(medv) ~ lstat, data = Boston)
summary(lm.fit)
par(mfrow=c(2,2))
plot(lm.fit)
```

## Higher order effect
``` {r echo=TRUE}
lm.fit5 <- lm(medv ~ poly(lstat, 5), data = Boston)
summary(lm.fit5)
plot(Boston$lstat, Boston$medv)
lines(seq(0,50,1), predict(lm.fit5,data.frame(lstat=(seq(0,50,1)))),col="green",lwd=2)
points(Boston$lstat, fitted(lm.fit5), col = "blue")
```

## Qualitative Predictors
We will now examine the `Carseats` data, which is part of the `ISLR` library. We will attempt to predict `Sales` (child car seat sales) in 400 locations based on a number of predictors.
``` {r echo=TRUE}
data(Carseats)
names(Carseats)
head(Carseats)
```

The `Carseats` data includes qualitative predictors such as `Shelveloc`, an indicator of the quality of the shelving location - that is, the space within a store in which the car seat is displayed - at each location. The predictor `Shelveloc` takes on three possible values, Bad, Medium, and Good. 
``` {r echo=TRUE}
lm.fit <- lm(Sales ~ ShelveLoc, data = Carseats)
summary(lm.fit)
```

## Qualitative + Quantitative
Given a qualitative variable such as `Shelveloc`, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.
``` {r echo=TRUE}
lm.fit <- lm(Sales ~ . + ShelveLoc:Price + Price:Age, data = Carseats)
summary(lm.fit)
```

## Coding for dummy variables
The `contrasts()` function returns the coding that R uses for the dummy variables.
``` {r echo=TRUE}
contrasts(Carseats$ShelveLoc)
```

# KNN regression
KNN method also can be used to predict `Sales` based on the `Carseats` data. First of all, load the package `FNN`. 
``` {r echo=TRUE}
library(FNN)
```

Suppose that we are interested to predict a test data point as follows.
``` {r echo=TRUE}
Carseats.test <- data.frame("CompPrice"=c(110,120),"Income"=c(70,100),
                     "Advertising"=c(10,20),"Population"=c(200,500),
                     "Price"=c(100,110), "ShelveLoc"=c("Bad", "Good"),
                     "Age"=c(20,50), "Education"=c(10,20), 
                     "Urban"=c("Yes","No"), "US"=c("Yes","No"))
print(Carseats.test)
```

Since the data contain some qualitative predictors, such as `Shelveloc`, those predictors need to be tranformed to be dummy variables, which can be done by `model.matrx` function.
``` {r echo=TRUE}
# combine and then transform together. Will separate them later
Carseats.all <- rbind(Carseats[,-1], Carseats.test) # -1 removing the output (Sales) column
X.all <- model.matrix( ~ ., data=Carseats.all)[,-1]
head(X.all)
```
The qualitative predictors now become numeric.

Moreover, standardization is also needed for KNN method, which can be done by `scale` function.
``` {r echo=TRUE}
X.all <- scale(X.all)
head(X.all)
```

We are ready to have *numeric* and *standardized* input data for KNN method.
``` {r echo=TRUE}
# Separate them to train and test (original size)
X.train <- X.all[1:nrow(Carseats),]
X.test <- X.all[-(1:nrow(Carseats)),]
```

KNN regression can be done by `knn.reg` function. Let's use 10 for now but the better selection of $K$ will be taught in Chapter 5.
``` {r echo=TRUE}
pred.out <- knn.reg(train = X.train, test = X.test, y = Carseats$Sales, k = 10)
print(pred.out$pred)
```